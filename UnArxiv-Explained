# "UnArxiv" ‚Äî Simplifying Research for Undergraduates

## Executive Summary

You're about to build **"UnArxiv"** ‚Äî a locally-run AI system that transforms dense, jargon-heavy research paper abstracts into clear, understandable summaries for first-year computer science students. Think of it as a "translator" that keeps all the meaning but removes the academic gatekeeping.

This isn't just a fine-tuning exercise; it's a **meaningful product** that will live on your machine and solve a real problem: the knowledge gap between cutting-edge AI research and undergraduate understanding.

### The Problem You're Solving

Picture this: You're an IITM student scrolling through Arxiv. You see a paper titled *"Efficient Sparse-Dense Matrix Multiplication Using Token Pruning in Transformer Attention Layers."* Your interest spikes. You click. You read the abstract:

> "We propose a novel approach leveraging structured sparsity and low-rank approximations to reduce computational complexity in attention mechanisms, achieving 2.3√ó speedup with <1% accuracy degradation on the GLUE benchmark."

Your brain: ü§î *"What is structured sparsity? What's a low-rank approximation? Why should I care about GLUE? Is this relevant to my interests?"*

You close the tab. Back to scrolling.

**This is the problem.** Research papers use compressed, technical language designed for experts. If you're not yet an expert, you're excluded‚Äînot because you're not smart enough, but because you haven't learned the dialect.

### Why Solve It This Way?

You could:
- Use ChatGPT or Claude ‚Üí But then your data goes to OpenAI's servers. Privacy? Gone.
- Bookmark every paper and read it all ‚Üí Time-consuming. Unsustainable.
- Take a semester-long course on research papers ‚Üí Too slow. You want to stay current *now*.

**What you're building instead:** A **private, local AI** that runs on your machine, offline, free. You own it. You control it. And it's fine-tuned specifically on the "abstraction task"‚Äîmaking complex ideas simple.

---

## The "Why" Behind Your Design Choices

### Why Qwen-2.5-3B (Not a Bigger Model)?

You have **8GB GPU VRAM**. You could *try* to squeeze a 7B model, but here's the reality:

| Model | Base Size (FP32) | With QLoRA 4-bit | Your Setup | Status |
|-------|------------------|------------------|-----------|--------|
| Llama 3.1 8B | ~30GB | ~8-10GB | ‚ùå Unstable, OOM crashes |
| Mistral 7B | ~26GB | ~7-9GB | ‚ö†Ô∏è Risky, slow due to memory swaps |
| **Qwen 2.5 3B** | **~11GB** | **~4-5GB** | ‚úÖ Safe, stable, room for gradients |
| Phi-3.5 Mini | ~14GB | ~5-6GB | ‚úÖ Also viable, slightly slower than Qwen |

The conventional wisdom says "bigger = better." But here's the plot twist: **Qwen-2.5-3B beats older 7B models on many tasks** [web:82]. Why? Because Qwen was trained *recently* with better recipes, better data, and optimizations the 7B models didn't have.

So you get:
- **Stability**: No mysterious crashes at 3 AM when training is halfway done.
- **Speed**: Your fine-tuning completes in hours, not days.
- **Performance**: A 3B model trained well on your task > a 7B model trained poorly due to memory constraints.

### Why Knowledge Distillation (The "Secret Sauce")?

Here's where most fine-tuning projects fail: **garbage data in, garbage model out.**

You could fine-tune Qwen-2.5-3B directly on raw Arxiv abstracts. It would learn to summarize. But would it learn to *simplify*? Probably not.

**Knowledge Distillation** is the cheat code [web:83][web:84]:

1. You take ~1,000-2,000 **difficult** Arxiv abstracts (from `ccdv/arxiv-summarization` dataset).
2. You send them to a **"Teacher" model** ‚Äî Google's Gemini 1.5 Flash or OpenAI's GPT-4o mini (the expensive smart model).
3. The teacher rewrites each abstract in **"high school English,"** explaining concepts, avoiding jargon.
4. You collect these pairs: `{Hard Abstract ‚Üí Simple Summary}`.
5. Your Qwen-2.5-3B (the "student") learns from these examples.

**Why this works:** The student doesn't just learn "summarization"‚Äîit learns the *specific style* of simplification you want. It learns that "transformer attention mechanism" becomes "the way AI models focus on different parts of input" or "a technique that lets AI decide what parts of information are most important."

This approach was used in actual research to create better summarizers with *smaller* models. You're using the same technique.

### Why Intel Arc + IPEX-LLM?

Your Intel Arc A750 is underutilized by most LLM tools. Why?

**NVIDIA GPU ecosystem** (CUDA) is the 800-pound gorilla. HuggingFace, PyTorch, everything defaults to CUDA.

**Intel's IPEX-LLM** is Intel's answer: optimizations specifically for Arc GPUs. Installing it unlocks:
- **Native 4-bit quantization** that actually works on Arc (not a hacky workaround).
- **XPU device** ‚Äî Intel's abstraction layer for GPU operations.
- **Memory-optimized kernels** that squeeze more performance from your Arc.

This is your **hidden edge**: Most people with Arc GPUs don't know about IPEX-LLM and give up. You'll have the knowledge to use it properly.

---

## Project Specifications

### Hardware & Constraints

**Your System:**
- **GPU**: Intel Arc A750 (8GB VRAM)
- **System RAM**: 16GB
- **Storage**: ~50GB free (for model, dataset, checkpoints)

**Memory Budget (The Hard Limit):**
```
GPU VRAM Allocation:
‚îú‚îÄ Qwen-2.5-3B (4-bit): ~4.5GB
‚îú‚îÄ LoRA adapters: ~0.3GB
‚îú‚îÄ Gradients & optimizer states: ~2GB
‚îú‚îÄ Activations (batch size 1, 512 tokens): ~0.8GB
‚îî‚îÄ Buffer (for safety): ~0.4GB
Total: ~8GB ‚úÖ (Fits perfectly)
```

If you exceed this, you'll hit OOM (Out of Memory) and the training will crash. **This is why the constraints below are non-negotiable.**

### The Model

**Base Model:** `Qwen/Qwen2.5-3B-Instruct`

Why this flavor (not the base `Qwen2.5-3B`)?
- The `-Instruct` variant is fine-tuned for following instructions. Since you want it to simplify *on demand*, this is better.
- It's a real model from Alibaba's Qwen team, actively maintained, and performs well on instruction-following tasks [web:72][web:73].

**LoRA Configuration** (The "Adapter"):

```python
r = 16              # Rank. Higher = more parameters, more VRAM.
                    # 16 is sweet spot: captures nuance without OOM.

lora_alpha = 32     # Scaling factor. Keeps training stable.

lora_dropout = 0.05 # Regularization. Prevents overfitting to your dataset.

target_modules = ["q_proj", "k_proj", "v_proj", "o_proj"]
                    # These are the "attention" layers where the magic happens.
                    # They're the neurons that decide what to focus on.

bias = "none"       # Don't add learnable biases. Saves memory.
```

**What LoRA does (in plain English):**
Instead of updating the entire 3B model (expensive, uses lots of VRAM), you add tiny "adapter" matrices. These adapters are 0.1% of the model size but are SO effective that fine-tuning just the adapters often matches fine-tuning the whole model [web:65]. It's like adding a thin lens in front of a camera instead of rebuilding the camera.

### The Dataset

**Stage 1: Raw Data**
- **Source**: `ccdv/arxiv-summarization` (Hugging Face dataset).
- **What it contains**: ~200k papers with abstracts. Fields: `abstract`, `summary` (paper's own summary from arxiv).
- **Your target**: CS papers only (~50k papers). Why? You want domain-specific knowledge.
- **Action**: Filter for `arxiv_categories` containing "cs." (Computer Science).

**Stage 2: Distillation (The Secret Sauce)**

This is where you create your **actual training data**:

1. **Select 1,500 abstracts** from the CS subset. Pick a mix:
   - Cutting-edge (NeurIPS, ICML papers from 2024-2025)
   - Classic (foundational papers that undergrads should know)
   - Diverse domains (NLP, Vision, Reinforcement Learning, Systems)

2. **Create the "Simplification Prompt":**
   ```
   You are an expert science communicator. Rewrite the following research 
   abstract for a first-year computer science student. 
   
   Rules:
   - Avoid jargon. If you must use technical terms, explain them.
   - Use analogies and real-world examples.
   - Explain WHY this research matters (not just WHAT it does).
   - Keep it under 150 words.
   - Make it engaging and curious-inducing.
   
   Original Abstract:
   [INSERT ABSTRACT HERE]
   
   Simplified Version:
   ```

3. **Hit the Gemini API** (or Claude, or GPT-4o mini):
   - Gemini 1.5 Flash: Free tier allows ~15M tokens/month. 1,500 abstracts √ó 200 words ‚âà 300k tokens. **You're well within free tier.**
   - Cost: $0 if you use free tier properly. üí∞

4. **Curate & Filter:**
   - Remove outputs that are too long or still too jargon-heavy.
   - Keep only high-quality simplifications (you're aiming for 1,000-1,200 good pairs).

**Example Dataset Entry:**
```json
{
  "instruction": "Simplify this research abstract for an undergraduate CS student, avoiding jargon but retaining all meaning.",
  "input": "We propose a novel approach leveraging structured sparsity and low-rank approximations to reduce computational complexity in attention mechanisms, achieving 2.3√ó speedup with <1% accuracy degradation on the GLUE benchmark.",
  "output": "This paper describes a way to make AI models' attention mechanisms (the part that decides what to focus on) faster and more efficient. By using mathematical tricks called 'sparsity' (making things simpler) and 'low-rank' (using fewer numbers), they make these parts 2.3 times faster with barely any loss in accuracy. They tested it on a standard AI benchmark and it worked great‚Äîuseful for running powerful AI on smaller computers."
}
```

**Total Dataset:** 1,000-1,500 JSON-formatted instruction-response pairs. Size: ~2MB.

### Training Configuration

**Key Hyperparameters** (Optimized for 8GB VRAM):

```
batch_size_per_device = 1
gradient_accumulation_steps = 4
    # Simulates batch_size = 4 without needing 4√ó VRAM
    # Batch size 4 is enough to learn patterns

learning_rate = 2e-4
    # Smaller than typical (1e-4 is minimum to learn anything)
    # Larger than minimum to prevent getting stuck in local minima
    # For LoRA, this sweet spot is 2e-4

max_seq_length = 512
    # Maximum tokens per example
    # 1 Arxiv abstract typically: 150 words ‚âà 200 tokens
    # 1 Simplified summary: 150 words ‚âà 200 tokens
    # Plus instruction: ~50 tokens
    # Total: ~450 tokens. 512 is safe with padding.

num_train_epochs = 3
    # Pass through the data 3 times
    # With 1,000 examples + gradient_accumulation=4:
    #   Effective updates = 1,000 / 1 √ó 4 √ó 3 = 12,000 steps
    #   Training time: ~4-6 hours (depending on Arc speed)

max_steps = 1000 (optional cap)
    # Stop after 1,000 training steps regardless
    # Helps prevent overfitting

save_strategy = "no"
    # Don't save checkpoints during training (saves disk I/O)
    # You'll save once at the end

fp16 = True
    # Use half-precision floats (16-bit instead of 32-bit)
    # Intel Arc handles this well
    # Cuts memory by ~50% with minimal accuracy loss

gradient_checkpointing = True
    # CRITICAL for 8GB VRAM
    # Stores activations on disk instead of RAM
    # Trades compute time for memory
    # Makes training 10-15% slower but doesn't OOM
```

**Estimated Training Time:**
- Dataset: 1,000 examples
- With batch accumulation: ~250 effective batches per epoch
- Qwen-3B on Arc (rough estimate): ~30 tokens/sec per example
- Total: **4-6 hours** for 3 epochs

---

## Technical Architecture

### The Pipeline (What Actually Happens)

```
Step 1: Load Qwen-2.5-3B (Quantized 4-bit)
  ‚îî‚îÄ> Model is loaded from HuggingFace Hub
  ‚îî‚îÄ> Immediately converted to 4-bit using IPEX-LLM's quantizer
  ‚îî‚îÄ> Moved to XPU device (your Arc GPU)

Step 2: Wrap with LoRA
  ‚îî‚îÄ> Adapter matrices are initialized (small, random)
  ‚îî‚îÄ> Only adapters will be updated during training
  ‚îî‚îÄ> Main model stays frozen

Step 3: Load & Tokenize Dataset
  ‚îî‚îÄ> Read JSONL file with {instruction, input, output}
  ‚îî‚îÄ> Combine into single text: "Simplify this abstract:\n[input]\n\n[output]"
  ‚îî‚îÄ> Tokenize to token IDs
  ‚îî‚îÄ> Pad/truncate to 512 tokens

Step 4: Training Loop
  For 3 epochs:
    For each batch of 1 example:
      ‚îî‚îÄ> Forward pass: Run Qwen on the tokens
      ‚îî‚îÄ> Compute loss: How wrong is the prediction?
      ‚îî‚îÄ> Backward pass: Calculate gradients
      ‚îî‚îÄ> Accumulate gradients (4 steps before update)
      ‚îî‚îÄ> Update only LoRA weights
      ‚îî‚îÄ> Log metrics (loss, learning rate, time)

Step 5: Save Adapters
  ‚îî‚îÄ> Save LoRA weights (~15MB)
  ‚îî‚îÄ> Save config (LoRA rank, target modules, etc.)

Step 6: Inference (Using Your Model)
  ‚îî‚îÄ> Load Qwen-2.5-3B
  ‚îî‚îÄ> Load LoRA adapters
  ‚îî‚îÄ> Merge adapters into the model (or use separately)
  ‚îî‚îÄ> Run on new abstracts:
      Input: "Simplify this abstract:\n[NEW PAPER ABSTRACT]\n\n"
      Output: [SIMPLIFIED SUMMARY]
```

### Why IPEX-LLM is Non-Negotiable

Standard PyTorch on Intel Arc requires **special installation and setup**. Most tutorials assume NVIDIA.

IPEX-LLM handles:
- **Device management**: Automatically detects Arc GPU and allocates correctly.
- **Kernel optimization**: Intel-specific computational shortcuts.
- **Memory pooling**: Reuses GPU memory efficiently (critical for 8GB).
- **4-bit quantization**: Stable, tested implementation on Arc.

Without IPEX-LLM, you'd be fighting library incompatibilities all week. With it, everything "just works."

---

## The "Why" Behind This Project (Deep Dive)

### Why Not Use an API?

You could call OpenAI's API every time you want a paper simplified. Why not?

1. **Cost**: Even cheaper models cost $0.01-0.05 per API call. You might simplify 100 papers a month. That's $1-5/month recurring. Over a year: $12-60. Your fine-tuning costs $0 after initial setup.

2. **Privacy**: Your paper abstracts are sent to OpenAI's servers. They might be used for training. Red flag for confidential research.

3. **Offline access**: API requires internet. What if Arxiv is down? What if you're on a flight? Your local model works anywhere, always.

4. **Customization**: With an API, you're stuck with OpenAI's "house style" of simplification. Fine-tune your own model ‚Üí you control the style completely.

### Why Not Just Prompt ChatGPT?

You could literally ask ChatGPT to simplify papers right now, for free (using the web interface). Why build something?

Because **learning the process is the point**. You're not just using ML; you're building it. This project teaches you:

- **Data engineering**: How to structure and curate datasets.
- **Model fine-tuning**: End-to-end: quantization, LoRA, training, inference.
- **Hardware optimization**: How to use specialized hardware (Intel Arc) efficiently.
- **MLOps**: Version control, experiment tracking, reproducibility.

These skills are what separate an "AI enthusiast" from an "ML engineer." Companies hire the latter.

## Success Metrics: How You'll Know It Works

### Qualitative Evaluation (The Real Test)

**Month 1 (Post-training):**

Pick 10 papers you've *actually wanted to read* from Arxiv in the past month. Run them through your model.

Ask yourself:
- Could I explain the main contribution to a first-year student with this summary? Yes/No
- Is the simplified version actually shorter and clearer? Yes/No
- Did the model invent details that aren't in the original? (This is a red flag. No=good)
- Would I cite this summary in a README or blog post? Yes/No

**Success threshold:** 7/10 papers with "Yes" to most questions.

### Quantitative Metrics (If You Want to Get Fancy)

Using ROUGE (Recall-Oriented Understudy for Gisting Evaluation) scores:

```
ROUGE-1: Unigram overlap (single words)
  Target score: >0.30 (your summary shares 30%+ vocabulary with reference)

ROUGE-L: Longest common subsequence
  Target score: >0.25 (captures sequence similarity)
```

You can compute these using the `rouge` Python library:

```python
from rouge_score import rouge_scorer

scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)

original_simple = "The way AI models process information"
your_output = "AI processes information through neural networks"

scores = scorer.score(original_simple, your_output)
print(scores)  # Will show your actual overlap
```

But honestly? If the qualitative test passes, the quantitative scores will follow.

### The Ultimate Test: Sharing It

**Month 2 onwards:**

Share your model on HuggingFace Hub (it's free). Post on Reddit, Twitter, etc.

If undergraduates or researchers actually *use* it and *cite* it in their own projects, you've built something real.

---

## The Reality Check: What Can Go Wrong?

**Honest potential issues:**

1. **OOM crashes during training:**
   - *Prevention*: Strictly follow the VRAM budget. If you get OOM, reduce `per_device_train_batch_size` to 1 or `max_seq_length` to 384.

2. **Poor simplifications (model outputs jargon):**
   - *Cause*: Distillation data quality is poor. Maybe the Gemini simplifications were still too complex.
   - *Fix*: Manually review and curate distillation outputs before training. Throw out the bottom 20% of quality samples.

3. **Training is slow:**
   - *Reality*: Arc GPU is slower than RTX 4090. Training in 4-6 hours (not 30 minutes) is normal.
   - *Workaround*: Be patient. Use this time to read papers, write documentation, or optimize dataset loading.

4. **Model overfits (works on training data, fails on new abstracts):**
   - *Prevention*: You have 3 epochs √ó 1,000 examples = only 3,000 gradient updates. Overfitting is unlikely unless your data is homogeneous.
   - *Prevention*: Keep `lora_dropout=0.05`. This regularizes learning.

5. **Installation headaches with IPEX-LLM:**
   - *Reality*: It's still emerging tech. Intel Arc support is good but not NVIDIA-level polished.
   - *Workaround*: Join the `LocalLLaMA` subreddit. The community is helpful.

---

## Timeline & Milestones

**Week 1: Setup**
- [ ] Install WSL2 Ubuntu, set up Python env, install IPEX-LLM
- [ ] Download `ccdv/arxiv-summarization`, filter for CS papers
- [ ] Test that you can load Qwen-2.5-3B and run a forward pass (inference)
- **Deliverable**: Model loads, you can ask it a question

**Week 2: Data**
- [ ] Write Python script to call Gemini API and generate simplifications
- [ ] Download 1,500-2,000 simplifications
- [ ] Format into JSONL training dataset
- [ ] Do quality check: manually read 30 examples, rate quality 1-5
- **Deliverable**: 1,000+ curated training examples in `train.jsonl`

**Week 3: Training**
- [ ] Implement training script with LoRA
- [ ] Dry run on tiny subset (10 examples) to catch bugs
- [ ] Run full training on full dataset
- [ ] Monitor training loss, save final checkpoint
- **Deliverable**: Trained LoRA adapters (15MB .safetensors file)

**Week 4: Evaluation & Cleanup**
- [ ] Evaluate on 10 new abstracts (qualitative test)
- [ ] Write inference script
- [ ] Create Gradio demo (simple UI)
- [ ] Upload model to HuggingFace Hub (public)
- [ ] Write README.md with usage instructions
- **Deliverable**: Public model on HuggingFace, shareable link

---

## What You'll Learn (Resume-Worthy Skills)

- **LLM Fine-tuning**: End-to-end: quantization, LoRA, training loops.
- **Knowledge Distillation**: Leveraging larger models to teach smaller ones.
- **Data Engineering**: Creating meaningful datasets from raw sources.
- **Hardware Optimization**: Efficient memory management on constrained GPUs.
- **Deployment**: Packaging, versioning, and sharing models.
- **Intel Software Stack**: IPEX-LLM, XPU device abstraction (rare expertise).

**Interview conversation starter:**
> "I fine-tuned a 3B parameter model for scientific text simplification using knowledge distillation. The main challenge was fitting everything into 8GB VRAM, so I used Intel's IPEX-LLM for optimized 4-bit quantization and gradient checkpointing. The model now simplifies Arxiv abstracts for undergraduates‚ÄîI published it on HuggingFace with full documentation."

That's not "I followed a tutorial." That's "I shipped a real project with constraints."

---

**Research is gatekept by language.** Papers are written in a compressed, dense format optimized for experts. This excludes curious people who haven't yet become experts.

This project is a **democratization tool**. It takes research from researchers and makes it accessible to students, non-specialists, even kids curious about AI.

In a world where AI access is becoming more equal, expertise in **localized, fine-tuned, private AI** is underrated. You're not just building a toy. You're building infrastructure for a future where intelligence is local, personalized, and owned by users‚Äînot corporations.

---

## Next Steps: What to Do Right Now

1. **Bookmark these URLs** (from your search results):
   - [IPEX-LLM GitHub](https://github.com/intel/ipex-llm)
   - [Intel PyTorch Extension Tutorial](https://christianjmills.com/posts/intel-pytorch-extension-tutorial/native-ubuntu/)
   - [IPEX-LLM Fine-tuning Demo](https://www.youtube.com/watch?v=MTK4UBccmLQ)

2. **Join communities:**
   - r/LocalLLaMA (Reddit)
   - r/IntelArc (Reddit)
   - HuggingFace Forums

3. **Start gathering data this week:**
   - Set up a Google Cloud account for Gemini API access.
   - Write a small script to test API calls.
   - Start downloading Arxiv data.

4. **Document as you go:**
   - Keep a GitHub repo. Even if it's just notes, version control it.
   - This becomes your portfolio.

---

## Final Thoughts

This project is ambitious but **achievable in 4 weeks**. The hardest part isn't the code; it's the patience to do things right.

You have:
- The hardware (Intel Arc A750 is underutilized, not weak)
- The skills (IITM DSA + Python experience)
- The motivation (you read Arxiv; this solves your problem)

The only thing left is to start.

Go build something meaningful. üöÄ

---

**Estimated Project Complexity:** ‚≠ê‚≠ê‚≠ê‚≠ê (Advanced, but structured)  
**Estimated ROI:** Very high (learning + portfolio + usable tool)  
**Expected Completion:** 4 weeks  
**Cost:** $0 (you have the hardware, Gemini free tier covers data)